{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75de205f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris \n",
    "iris_dataset = load_iris() # load_iris is a Bunch object, which is similar to a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b789183f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('keys of iris dataset: \\n{}'.format(iris_dataset.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603a5ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_names is an array of strings\n",
    "print('Target names: {}'.format(iris_dataset['target_names']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67e479e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the value of feature_names is a list of strings, giving the description of each feature\n",
    "print('Feature names: \\n{}'.format(iris_dataset['feature_names']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9ae512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data itself is contained in teh target and data fields\n",
    "# data contains teh numeric measurements of sepal length, sepal width, petal length, and petal\n",
    "# width in a NumPy array\n",
    "print('Type of data: {}'.format(type(iris_dataset['data'])))\n",
    "print('Shape of data: {}'.format(iris_dataset['data'].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae62cb3a",
   "metadata": {},
   "source": [
    "Training and Testing Data\n",
    "To assess the model's performance, we must show it new data for which we have labels. This is usually done by splitting the labeled data we have collected into two parts. One part of teh data is used to build our machine learning model, and is called the training data or training set. The rest of the data will be used to assess how well the model works; this is called teh test data, test set, or hold-out set.\n",
    "\n",
    "scikit-learn contains a function that shuffles the dataset and splits it for you: teh train_test_split function. This function extracts 75% of the rows in teh data as the training set, together with teh corresponding labels for this data. The remaining 25% of the data, together with teh remaining labels, is delared as the test set. Deciding how much data you want to put into the training and test set respectively is somewhat arbitrary, but using a test set containing 25% of the data is a good rule of thumb.\n",
    "\n",
    "in scikit-learn, data is usuallyt denoted with a capital X, while labels are denoted by a lowercase y. We use capital X because the data is a two-dimensional array (a matrix) and the lowercase y because the target is a one-dimensional array (a vector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b282346",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris_dataset['data'],iris_dataset['target'], random_state=0)\n",
    "print('X_train shape: {}'.format(X_train.shape))\n",
    "print('y_train shape: {}'.format(y_train.shape))\n",
    "print('X_test shape: {}'.format(X_test.shape))\n",
    "print('y_test shape: {}'.format(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b342929",
   "metadata": {},
   "source": [
    "Looking at your data\n",
    "One way to look at your data is to use a scatter plot. We can look at multiple dimensions of data by creating a pair plot, which looks at all possible pairs of features. If you have a small number of features, such as the four we have here, this is quite reasonable. \n",
    "\n",
    "To create the plot, we first turn the NumPy array into a Pandas DataFrame. pandas has a function to create pair plots called scatter_matrix. The diagonal of this matrix is filled with histograms of each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5df9e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.plotting import scatter_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import mglearn\n",
    "from IPython.display import display\n",
    "\n",
    "# create dataframe from data in X_train\n",
    "# label the columns using the strings in iris_dataset.feature_names\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris_dataset['data'],iris_dataset['target'], random_state=0)\n",
    "iris_dataframe = pd.DataFrame(X_train, columns=iris_dataset.feature_names)\n",
    "\n",
    "# create a scatter matrix from teh dataframe, color by y_train\n",
    "scatter_matrix(iris_dataframe, c=y_train, figsize=(15, 15), marker='o', hist_kwds={'bins':20}, s=60, alpha=.8, cmap=mglearn.cm3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8573c35e",
   "metadata": {},
   "source": [
    "Building your first model: K-Nearest Neighbors\n",
    "Building this model only consists of building the training set. To make a prediction for a new data point, the algorithm finds the point in teh training set that is closest to the new point. Then it assigns the label of this training point to the new data point.\n",
    "\n",
    "The k-nearest neighbors classification algorithm is implemented in teh KNeighborsClassifier class in teh neighbors module. Before we can use the model, we need to instantiate the class into an object. We will set the parameter of the object (k-neighbor number) to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b75ee27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "# we're going to build a model on the training set by calling the fit method, which takes a NumPy array as argument\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# build some new data (create a NumPy array)\n",
    "X_new = np.array([[4,2.9,1,0.2]])\n",
    "\n",
    "# now we use our KNN object to make a prediction\n",
    "prediction = knn.predict(X_new)\n",
    "print('Prediction {}'.format(prediction))\n",
    "print('Predicted target name: {}'.format(iris_dataset['target_names'][prediction]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bc5787",
   "metadata": {},
   "source": [
    "Evaluating the Model\n",
    "We know what the correct species is for each iris in the test set. Therefore, we can make a prediction for each iris in the test data and compare it against its label (the known species). We can measure how well the model works by computing the accuracy, which is the fraction of flowers for which the right species was predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d1c976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to print out the predictions our model makes based on our test input data\n",
    "y_pred = knn.predict(X_test)\n",
    "print('Test set predictions:\\n {}'.format(y_pred))\n",
    "\n",
    "# manually calculate the score\n",
    "print('Test set score: {:.2f}'.format(np.mean(y_pred == y_test)))\n",
    "\n",
    "# we could also use the score method of the knn object, which will compute the test set accuracy for us\n",
    "print('Test set score: {:.2f}'.format(knn.score(X_test,y_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
